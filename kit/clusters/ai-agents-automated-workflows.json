{
  "number": 7,
  "name": "AI Agents & Automated Workflows",
  "slug": "ai-agents-automated-workflows",
  "description": "You can use AI agents to coordinate tasks, monitor information and assist editorial workflows. The key is to do this without surrendering judgment or too much control. AI agents are a step beyond single-task AI tools. Instead of responding to one prompt and stopping, an agent can be instructed to pursue an objective over time: checking sources regularly, combining multiple tools, maintaining memory and triggering actions when conditions are met. If a large language model is best thought of as a very capable “brain” that answers questions when you speak to it, an AI agent is that same brain given a role, a goal, and permission to act repeatedly in the background. This is the moment when you may think this sounds like prompting. It is important to distinguish an AI agent from simply giving a large language model a role in a prompt. A prompted LLM is reactive: it responds once, with no persistence, no awareness of time, and no ability to act unless a human asks again. An agent, by contrast, is a system built around the model that runs continuously or on defined triggers. This feature of understanding time is important. It has external memory, can check the same sources repeatedly, decide when something has changed. The language model provides reasoning, but the agency comes from the surrounding infrastructure (schedules, state, tools and thresholds) not from the prompt itself. Instead of waiting for a prompt, an agent runs in a loop: it checks what it is supposed to watch, decides whether anything has changed, uses tools if needed (search, databases, scrapers, internal archives), remembers what it has already seen, and only surfaces something to a human when it meets a defined threshold. In practice, this makes an agent feel less like a chatbot and more like a junior researcher or assistant who has been given standing instructions and reports back when something genuinely matters. Here are some examples of what agents are capable of: • An agent can monitor court registries or procurement portals and alert reporters when new documents appear. • An agent can be used to cross-reference names across previously published stories, leaked datasets and company registries. • An agent can track whether published articles are being republished, plagiarized or misquoted elsewhere online. • Maintain a “beat assistant” trained on a specific topic (e.g. energy policy, housing law). • Run structured checks on draft articles (e.g. “flag unclear sourcing”, “identify unverifiable claims”). • Interrogate large internal document collections repeatedly without re-prompting from scratch. Agents can influence agenda-setting, tip discovery, and the allocation of human attention. Used well, they create time for reporters to do good work. Used poorly, they create opaque systems that newsroom staff no longer fully understand.",
  "teaching_guidance": "When chatting to your students, emphasise that agents do not replace editorial judgment, they reallocate attention. Students should understand that deciding what an agent watches is itself an editorial act. And be sure, when talking to your students, to stress the difference between automation which is safe and predictable and autonomy which is unpredictable and risky. Also, when talking to students, stress that they need to map workflows before introducing agents. This can be even with a pen and paper. And this is good advice for vibe-coding as well. Even if AI can handle the details and code, it still struggles with pulling out to the macro. So, tell students if they cannot explain a process clearly, they are not ready to automate it. Students should leave this cluster understanding: 1. Agents can appear consistent while being wrong. When multiple agents agree or reinforce each other’s outputs, errors can look more credible rather than less. Journalists must recognise false consensus and step in before agreement is mistaken for verification. 2. Agents follow instructions, not intent. An agent will execute rules exactly as defined, even when circumstances change or ethical context shifts. Journalists must recognise when rigid automation is producing technically correct but editorially inappropriate results and intervene.",
  "exercises": [
    {
      "number": 1,
      "title": "Define the handover point",
      "description": "Students are given a short description of an agentic workflow (e.g. an agent that monitors council agendas and flags agenda items that match predefined keywords). Students must identify: • What the agent is allowed to do on its own • The exact moment a human must take over and this speaks to the point above around needing to design workflows (even if just on pen and paper) before you can start automating and part of this is knowing where these handover points are. • Where is the point that should force human review. Learning outcome: Teaches boundary-setting and intervention points without system design."
    },
    {
      "number": 2,
      "title": "Map a failure to a decision",
      "description": "Students are given a brief failure scenario. For example, an agent misses a late-posted document or flags a routine item as urgent. Students need to determine: • What decision the agent made correctly • What decision it made incorrectly • Which human decision could have prevented harm Learning outcome: Shifts focus from “what went wrong” to “where responsibility sits”."
    }
  ],
  "where_to_start": "Start with: Claude Projects or Custom GPTs. Why: Low technical barrier, immediate usefulness. What it teaches well: How persistent context changes analysis. Why instructions matter more than prompts. Best teaching use: Investigations, policy reporting, archive work. Introduce next: n8n (selectively). Why: Shows how agents are actually built in practice. What it teaches well: Systems thinking, accountability, and auditability. Avoid initially: Fully autonomous agents. Why: They teach the wrong lessons first, spectacle over responsibility. Addendum A: Tools with Risks 1) The Transcription Trap In the high-pressure environment of a newsroom, the transcription of interviews is a bottleneck. For decades, this was a manual, labour-intensive process. The arrival of AI has promised to reclaim hours of productivity. However, this convenience (in certain cases and with certain tools) comes with a severe hidden cost: the loss of custody over source data. Otter.ai Tool Overview: Otter.ai is the market leader in consumer AI transcription, known for its \"OtterPilot\" feature that automatically joins meetings to record and transcribe conversations. What is the risk? The primary risk with Otter.ai is its architecture of aggressive data ingestion. The \"OtterPilot\" bot can join meetings automatically if calendar permissions are granted, potentially recording off-the-record pre-ambles or conversations where not all parties have consented. 116 In 2024 and 2025, the company faced class-action litigation117 in the United States alleging violations of wiretapping laws, specifically regarding the non-consensual recording of voice biometrics. The data resides on US-based servers, subject to foreign subpoenas. And more critically, the privacy policy allows for the use of audio data to train the company's AI models. This means a whistleblower's voice is processed, tokenized, and potentially retained within the model's training corpus. 2) The Writing Process Grammarly Tool Overview: A cloud-based writing assistant that checks spelling, grammar, and tone. Grammarly functions by transmitting every keystroke in an active text field to its cloud servers for analysis. For a journalist investigating state corruption or organized crime, this is an operational security failure. The text of a draft investigation, containing names, dates, and allegations, is exposed to Grammarly's infrastructure before it is even published. lessons-from-the-otter-ai-class-action-complaint/ What is the risk? The 2024–2025 integration of generative AI features increases the likelihood that user data feeds into broader language models. 118 Universities and corporations with high security requirements often ban Grammarly for this exact reason. 3) Visual Journalism Visual storytelling is dominated by short-form video (TikTok, Reels). The tools required to compete in this algorithmic arena—offering auto-captions, filters, and snappy transitions are predominantly mobile-first and data-hungry. The risk here is biometric harvesting. CapCut Tool Overview: The most popular mobile video editor, owned by ByteDance (parent of TikTok). What is the risk? CapCut is a consumer video-editing application whose data collection and licensing practices have raised significant privacy and surveillance concerns among regulators and legal commentators. In 2024-2025, it faced class-action lawsuits in the US (Illinois) for violating the Biometric Information Privacy Act (BIPA). 119 The lawsuits allege that CapCut collects biometric identifiers, including facial and voice data, as well as device and location information, without obtaining consent required under Illinois law. 120. CapCut updated its Terms of Service on June 12, 2025, which have drawn legal and public criticism121 because they grant broad rights to user-generated content, including rights to use, edit, reproduce, and exploit content worldwide, with implications for biometric features (these include facial recognition and voice patterns). This update expanded the scope of what CapCut can legally do with uploaded content without separate user consent. For a journalist in the CEE region, the geopolitical risk is acute. ByteDance is subject to Chinese security laws.122 4) The Communication Infrastructure For certain parts of the world, Telegram is not just a messenger app, it is the internet. It is the primary source of news, alerts, and social organization. However, for a journalist, relying on it for private communication is a dangerous fallacy. bytedance-data-collection Telegram Tool Overview: The fatal flaw of Telegram is that Cloud Chats are not End-to-End Encrypted by default. Telegram holds the decryption keys. 123 This means the company, and by extension, any entity that can coerce the company, can read the messages. Only “Secret Chats” are true end-to-end encrypted (E2EE) and must be manually enabled124. The Better Alternative: Signal Tool Overview: An encrypted messenger owned by a non-profit foundation. Signal minimizes metadata retention 125 . Even if subpoenaed, Signal can typically only produce the date of account creation and the date of last login. It is the gold standard for contact. Addendum B: AI & The Strategic Reinvestment of Time The most dangerous outcome of AI adoption is the Jevons Paradox: where increased efficiency simply leads to increased consumption (or in this case, production of low-value content), leaving staff just as burned out as before. If AI saves a reporter 5 hours a week, and the newsroom simply demands 5 more articles of the same quality, the opportunity for transformation is lost. The strategic goal of AI in the newsroom is not to reduce headcount (fire staff) but to upgrade the value of human labour. 126 This section outlines a strategy for the \"AI Dividend\". This is the deliberate reinvestment of saved time into high-value activities that bolster the newsroom's offering. Quantifying the Dividend The Thomson Reuters 2024 Future of Professionals report indicates that AI tools can save professionals approximately 12 hours per week 127 by 2029. The report likened this to adding a colleague for every 10 team members, highlighting how productivity gains can aggregate across an organisation. The \"Stop Doing\" List: A Prerequisite for Reinvestment Reinvestment cannot happen without a deliberate cessation of low-value tasks 128. Leadership must sanction a \"Stop Doing\" list to create the capacity for new work. This requires a rigorous audit of current newsroom activities. The critical management challenge is tracking this time and ensuring it is not absorbed by \"shallow work\" (emails, Slack, browsing). Strategic Reinvestment Areas Strategy 1: Investigative & Enterprise Time saved from churning out \"commodity news\" (press release rewrites, basic event coverage) should be directed toward original reporting. ● If a reporter saves 5 hours a week on transcription, mandate that those 5 hours are blocked off for time dedicated solely to long-term investigation, source building, or public records requests. Strategy 2: Community Listening and Engagement AI cannot empathize. It cannot have coffee with a source, attend a town hall just to \"read the room,\" or comfort a grieving family member. ● Reinvest time into physical presence. Send reporters out of the newsroom not just to cover stories, but to find them. Organize town halls, listener panels, and meet-and- greets. ● Trust is rebuilt through human connection. If AI handles the digital noise, humans must handle the analog signal. This creates a \"moat\" around the publication that AI aggregators cannot cross. Strategy 3: New Product Development Use the technical capacity built (via Section 3) to launch new editorial products that were per-week-by-2029 doing-list-not-a-to-do-list/ previously too resource-intensive. ● Launch a hyper-local newsletter, a niche podcast, or a data dashboard tracking local economic indicators. ● These products drive subscriptions and loyalty, moving the business model away from reliance on high-volume, low-value programmatic ad traffic. With AI assisting in the production (e.g., summarizing articles for a newsletter), a small team can support a product that previously required a large staff. Addendum C: Futureproofing & Updating the Curriculum The AI Toolkit is intentionally not a fixed list of tools. AI systems change frequently. Features are added or removed, pricing models shift, companies are acquired and data-use policies evolve. This addendum provides a practical, low-burden approach to curriculum maintenance that does not require technical expertise. 1. A Layered Approach to Curriculum Updates Rather than constant monitoring, educators are encouraged to use a layered update cycle that balances awareness with sustainability. Ongoing informal awareness Educators should maintain general awareness of changes in the AI and journalism landscape through everyday signals: • Colleagues mentioning new tools or features • Students referencing emerging platforms • Headlines, social media posts, or conference discussions These informal cues are often the earliest indicators that something has changed. At this stage, no action is required beyond noticing. Monthly Once per month, educators should conduct a short review (15–30 minutes) focused on the tools actively referenced in their teaching. Key questions include: • Have any tools changed pricing, access levels, or ownership? • Have any tools been discontinued, merged, or significantly altered? • Have there been notable changes to data-use or privacy policies? Some institutions now formalize this as a brief “tool sanity check” to catch disruptions before they affect students. Annual deep review Once per year, a deeper review should be undertaken to reassess the toolkit as a whole. This is the appropriate moment to: • Retire tools that are obsolete, unstable, or no longer trustworthy • Introduce tools that have matured and proven their value • Revisit ethical, legal, and data-protection considerations • Update warnings, caveats, or teaching notes where needed 2. Tracking Tools and Trends Responsibly Educators do not need to track the entire AI ecosystem. The focus should remain on tools that serve journalistic work and align with editorial values Example of a tool that compares LLMs LLM Arena (also known as LMArena, formerly Chatbot Arena) is a public, crowd-sourced platform for evaluating and comparing large language models (LLMs) 129. On the platform, users can submit the same prompt to two anonymous AI models and vote on which response they think is better. These pairwise comparisons are aggregated into a leaderboard using an Elo- style rating system, allowing models to be ranked by performance based on real human preference data rather than just static metrics. Monitoring tool stability Simple habits are sufficient: • Subscribe to one or two general AI or journalism newsletters • Set basic alerts for the names of tools used in class • Bookmark official blogs or status pages for key tools and review them periodically If a tool loses functionality, changes its business model, or disappears, it should be reassessed or removed from teaching materials. Evaluating new tools When a new AI tool comes to attention, educators should assess it using three criteria: 1. Usability. Can students and educators learn it quickly? 2. Relevance. Does it support real journalistic tasks such as research, writing, editing, or verification? 3. Responsibility. Does it respect privacy, consent, accuracy, and transparency? Educators are encouraged to test tools briefly using classroom-relevant examples rather than relying on marketing claims. Popularity alone is not a reason to include a tool. Maintaining trust and compliance Educators should remain alert to: • Changes in how tools handle user data • New requirements for uploads, permissions, or third-party sharing • Shifts from free access to paid or restricted models If a tool’s data practices change in ways that conflict with classroom expectations, its use should be paused and reconsidered. Maintaining trust with students is essential. Addendum D: Resources & Research Database A curated collection of the most recent and impactful research papers, policy briefs, and training courses regarding AI in the media, updated for the 2025 academic year. 1. Selected Research on the Impact of AI on Media ● The State of The Bots (Q1 2025). \"RAG bot scrapes now exceed Training bot scrapes.\" 130 A critical report on how AI bots are aggressively scraping news publisher data, often ignoring robots.txt protocols. ● AI Across America: Attitudes On AI Usage (August 2025) 131. A massive survey (Report #116) revealing that 50% of US adults now use AI tools, with significant data on public trust and fear of job displacement. ● Generative AI Outlook Report (2025)132 . European Commission JRC. Maps the path for AI policy and innovation in Europe, warning that without ethical oversight, the EU risks falling behind. ● Artificial Intelligence in Eastern Africa Newsrooms (March 2025) – CIPESA.133 An analysis of perceived prevalence, ethics, and impact of AI on media freedom in Ethiopia, Kenya, Tanzania, and Uganda. ● Generative AI & Journalism: Content, Perceptions, and Audiences (Feb 2025) – RMIT University. 134 A study across seven countries (including Germany and UK) examining regulation further-policy-action how audiences perceive AI-generated news content and the transparency paradox. ● Generative AI in Journalism: The Evolution of Newswork (April 2024) – Associated Press. 135 The definitive report on how AP and other major newsrooms are adapting ethical guidelines for the generative age. ● Hallucination Mitigation Using Agentic AI (Jan 2025). 136 Technical research on using multiple AI agents to check each other's work to reduce \"hallucinations\". 2. Policy, Ethics & Legal Frameworks ● AI in the Work of an Attorney-at-Law (2025).137 Recommendations on how legal professionals (and by extension, investigative journalists dealing with legal data) should use AI tools securely. ● M20 Policy Brief 4: AI, Africa and the G20 (July 2025) A critique of the \"power concentration\" of AI in the Global North and recommendations for protecting African data sovereignty. 138 ● The Paris Charter on AI and Journalism (Nov 2023). 139 A foundational ethical document signed by 16 partner organizations (RSF) defining principles for AI in news. ● Exposing Technology-Facilitated Gender-Based Violence (2023) – UNESCO. 140 How generative AI is being weaponized to harass women journalists and public figures. 3. Online Courses & Educational Resources ● Google News Initiative: Pinpoint & Data Tools. Using AI to search through thousands of PDFs and handwritten documents.141 ● MIT: Introduction to Deep Learning (6.S191). A technical deep dive for those wanting to understand the architecture behind the models. 142 4. Tools & Open Source Repositories ● Hugging Face: Journalists' Toolkit143 . Finding open-source models to run locally for ceptions_and_Audience_Experiences/28068008 the-g20/ privacy. ● Journalism Cloud Alliance (OCCRP/GFMD)144 . Shared infrastructure for investigative newsrooms to lower cloud costs and increase security. 5. Organisations Leading AI Innovation in Global South ● Centre for Information Integrity in Africa (CINIA).145 Combatting disinformation and safeguarding democratic values in the African digital space. ● Vambo AI.146 Bridging the language gap with AI solutions for indigenous African languages. ● SADiLaR (South African Centre for Digital Language Resources). 147 Creating digital resources for under-resourced languages. ● Lelapa AI. \"AI for Africans, by Africans\" building resource-efficient language models. 148 6. Podcasts & Multimedia ● Reuters Institute: Digital News Report 2025 Launch (Africa). Discussion on trust, AI usage, and news consumption trends 149 . ● Inside the FT’s AI-Powered Story Finding. 150 How the Financial Times uses AI not just to write, but to find stories in data. 7. Legacy Research (Foundational Reading) ● New Powers, New Responsibilities (2019) – LSE Polis. The seminal global survey that launched the JournalismAI initiative.151 ● News Automation: The Rewards, Risks and Realities (2019) – WAN-IFRA. An early but relevant guide to the practicalities of \"robot journalism.\" 152 ● The Next Wave of Disruption (2021) – IMS. Emerging market media use of AI journalism/ 153 Addendum E: Who to follow online - Nicholas Diakopoulos 154 . Professor at Northwestern University and Director of the Computational Journalism Lab (CJL) and their newsletter, Generative AI in the Newsroom 155 . - Ethan Mollick156. Excellent at translating generative AI into practical newsroom and storytelling workflows. - Charlie Beckett 157. Director at LSE’s Polis; writes clearly about AI, power, trust, and journalism ethics. - Rasmus Kleis Nielsen158 . Focuses on how audiences, platforms, and AI reshape journalism economics and trust. - Bettina Peters 159 . Strong on newsroom AI adoption, governance, and real-world constraints. - Simon Rogers160. Deep experience in data-driven storytelling and automation at scale. - Meredith Broussard 161. Essential counterweight to hype; strong on why AI often fails journalists and communities. - Anya Schiffrin 162. Focuses on media sustainability, regulation, and AI’s political consequences. - Chris Moran 163. One of the best public thinkers inside a major broadcaster on AI experimentation. use-of-artificial-intelligence-and-machine-learning - Emily Bell 164. Columbia Journalism School; essential reading on platforms, AI, and media power.",
  "tool_slugs": [
    "chatgpt-atlas",
    "apify",
    "autogen",
    "n8n",
    "zapier-ai"
  ],
  "tool_count": 5
}