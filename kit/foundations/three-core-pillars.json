{
  "slug": "three-core-pillars",
  "title": "Three Core Pillars of the Curriculum",
  "content": "Three Core Pillars of the The foundational principles that guide all AI use in this\nCurriculum toolkit: sovereignty, reinvestment of time, and balance.\nA framework for evaluating AI tools before using them\nThe AI Audit Rubric\nin journalistic work or teaching.\nTool Cluster 1: Transcription & Using AI to process interviews, recordings, and\nTranslation multilingual material accurately and safely.\nTool Cluster 2: Verification & Tools and techniques for verifying content, tracking\nInvestigations changes, and uncovering networks.\nTool Cluster 3: Writing & Using AI to support drafting, summarising, and working\nAnalysis with documents and data.\nTool Cluster 4: Audio, Video & Applying AI to multimedia storytelling and social\nSocial distribution while maintaining editorial control.\nTool Cluster 5: Security Understanding the security risks of AI tools and how to\nChallenges protect data, sources, and journalists. Tool Cluster 6: Building Your Using AI-assisted coding to build simple newsroom\nOwn Tools (“Vibe Coding”) tools without relying on vendors.\nUsing AI agents to coordinate tasks, monitor\nTool Cluster 7: AI Agents &\ninformation, and assist editorial workflows.\nAutomated Workflows\nAddendum A: Caution List: A cautionary list of commonly used tools that pose\nTools with significant risks significant ethical, legal or security risks.\nAddendum B: Strategic How time saved through AI should be reinvested into\nReinvestment of Time better journalism rather than more output.\nAddendum C: Resources & Key readings, policies, and research for further\nResearch teaching and study.\nAddendum D: Resources &\nResearch Database\nAddendum E: Who to follow\nonline\nCopyright Notice\n© 2025 Thomson Reuters Foundation and/or Reuters Foundation Consultants Limited. All rights\nreserved. This Toolkit is provided for personal, educational, and non-commercial use only. Any\nother use requires prior written permission from Thomson Reuters Foundation or Reuters\nFoundation Consultants Limited.\nDisclaimer\nThe Toolkit contains links and references to resources provided by third parties. These links are\nprovided for informational purposes only and do not constitute professional advice. While the\nToolkit may be used for teaching or training, it should not be relied upon as a sole source of\nguidance. Although we make reasonable efforts to source reliable content from third parties, we do\nnot guarantee the accuracy of, nor do we endorse, the views, recommendations, or opinions\nexpressed by any third-party content provider linked in the Toolkit. We also have no control over the\ncontent of those links or resources.\nWe accept no liability for any decisions made or actions taken by readers based on this Toolkit. You\nshould not act or refrain from acting based on the information in the Toolkit without first seeking\nappropriate professional advice regarding your specific facts and circumstances.\nObjectives & learning outcomes Objectives\n1: Equip lecturers to teach AI as a set of editorial, ethical and security decisions that shape\njournalistic practice.\n2: Build critical AI literacy grounded in journalistic judgment by enabling students to evaluate,\naudit, and challenge AI systems, with particular attention to verification, bias and hallucination.\n3: Strengthen data independence and source protection in AI workflows by introducing student\njournalists to open-source and local alternatives that reduce dependency on global platforms\nand protect sensitive material in high-risk environments.\n4: Reframe efficiency gains as a reinvestment opportunity into investigation, verification,\naudience engagement and human reporting.\n5: Develop the capacity for student journalists to build, adapt and critique simple newsroom\ntools using AI-assisted coding, reducing reliance on vendors, and increasing editorial\nindependence.\nLearning outcomes\n1: Apply structured frameworks, like the Cost, Difficulty, Invasiveness (CDI) scores1, to evaluate\nAI tools for cost, difficulty, invasiveness, security and contextual relevance before use.\n2: Select appropriate AI tools and workflows based on the sensitivity of the material, the\nvulnerability of sources and the potential harm of misuse.\n3: Identify hallucinations, translation distortions, analytical weaknesses and framing bias in AI-\ngenerated text, audio, visuals and code.\n4: Demonstrate editorial accountability when using AI by explaining how AI was used in\nreporting or production, including verification steps, human oversight, and disclosure decisions.\n5: Prototype simple, purpose-built newsroom tools with clear editorial intent using AI-assisted\ncoding to build or adapt small tools while articulating their journalistic value, limitations, and\nethical risks.\nIntroduction\nWe are at an inflection point in journalism education. By 2025, the challenge is no longer access\nto Artificial Intelligence, but the governance of it. Research indicates that over 42% of journalists\nare already using \"Shadow AI\" (unapproved tools used without editorial oversight) to write\n1 Created for this toolkit headlines, transcribe interviews, and generate code.2\nYour students are likely already using AI tools. The danger is that they are using them blindly.\nThey could be feeding sensitive data into opaque models, risking source confidentiality and\nrelying on \"black box\" algorithms. This toolkit is designed to help you elevate your teaching from\nsimple \"tool training\" to critical AI literacy. It does not merely list software. Here our goal is to\ntrain journalists who are not just users of technology, but auditors of it.\nArtificial Intelligence is already embedded in journalism, whether it is formally acknowledged or\nnot. Students and early-career journalists are using AI systems to transcribe interviews,\nsummarise documents, translate material, write code, generate visuals, and search for\ninformation. The question facing journalism education is therefore not whether AI should be\nintroduced, but how it should be understood, governed and used responsibly.\nIn the newsroom, AI is currently used in six broad ways.\n1) It is used to support reporting and research by transcribing interviews, translating\nmaterial, and searching large document collections.\n2) It is used in verification and investigations to analyze images and videos, track changes\nto official sources and identify networks of actors.\n3) It is used in writing and summarising long texts, cleaning data and assisting with drafting.\n4) It is used in audio, video and visual storytelling to improve accessibility and reach\naudiences in new formats.\n5) It is used in security and source protection through encryption, document sanitization\nand local processing.\n6) It is increasingly used to build small, custom tools and this allows journalists to create\ntheir own scrapers, calculators, or explainers without relying on external vendors.\nHow to Use This Toolkit\nThis toolkit is designed as a teaching companion. It assumes that lecturers are working with\nstudents who may be curious, sceptical or already experimenting independently with AI tools.\nRather than organising teaching around tools themselves, the toolkit is structured around\njournalistic problems. Each tool cluster begins with a real newsroom challenge, for example,\nhandling multilingual interviews, verifying social media content, or analysing messy\ngovernment data. Tools are introduced only as responses to these problems, not as ends in\nthemselves.\nfinds/ For teaching purposes, it is important to demystify AI early. Artificial Intelligence is not a single\nsystem and it does not “think 3.” It is a collection of techniques that recognise patterns in data\nand generate outputs based on probability. In journalism, this means AI is very good at handling\nscale (large volumes of text, audio, images or data) and very poor at understanding meaning,\nintent, harm or truth. This imbalance is central to how it should be taught.\nThroughout the toolkit, lecturers are encouraged to slow students down rather than speed\nthem up. Every tool is accompanied by a Cost Difficulty Invasiveness (CDI) score 4, which\nhelps students understand trade-offs. A tool that is easy to use may expose sensitive data. A\nfree tool may monetize user content. A powerful tool may quietly train on journalistic work.\nThese considerations are treated as editorial decisions, not technical details.\nA central concept used throughout the toolkit is the Time Dividend 5. AI saves time, but time\nsaved is not neutral. If reclaimed time could be used to simply produce more content faster\nand then journalistic quality erodes. The toolkit therefore pairs each tool with how time will be\nsaved and then suggestions for how saved time should be reinvested into deeper reporting,\nverification, source development or audience engagement. This reinforces the idea that AI’s\nvalue lies in strengthening journalism, not hollowing it out.\nUsing the term AI can be fraught at the best of times. It is unavoidable to use it throughout\nthis toolkit, but using terms like “an AI” often makes people think of AI as a sentient agent or\na conscious being 6. It also implies there's one discrete object being called AI, which isn't\naccurate. When talking to your students, try to make sure that all references position AI as a\nfield of practice or as a specific tool, not as a grander intelligence. AI works with patterns, not\nmeaning7, and it is important to stress that.\nFinally, the toolkit is intentionally comparative. Wherever possible, commercial tools are\nshown alongside open-source or locally run alternatives. This allows students to understand\nwhat convenience buys them (and what it costs them) and reduces dependency on a small\nnumber of global platforms. In regions where resources are limited or security risks are high,\nthis comparative approach is essential.\nThe core pillars of this toolkit:\n3 AI Is Not Intelligent (research article). Statistical models excel at pattern recognition but lack essential\nhuman intelligence traits, 27 Jan 2025.\nhttps://www.researchgate.net/publication/388410056_AI_Is_Not_Intelligent\n4 Created for this toolkit\n5 Created for this toolkit\n7 Artificial Intelligence Works With Patterns, Not Meaning. Medium (2025). https://medium.com/core-\nai/artificial-intelligence-works-with-patterns-not-meaning-and-that-changes-everything-e736da3250c9 ● 1) Sovereignty 8. Whenever possible, use tools that keep data on the journalist's own\ndevice. The important note to keep in the back of your head at all times is that using AI\ncan always results in a security risk and that is why when deciding to use AI it isn’t just a\ndecision to improve efficiency, but also a decision that takes security into account 9. AI\nsovereignty is about allowing the media to control its own data pipelines, train and\noperate its own AI models that reflect local context, build their own tools and run them\non their own infrastructure. They need this to save money, to make money and also to\nnot get swallowed by companies that don't have their best interests at heart. It is a way\nfor countries or communities to operate without global reliance on big tech. For our\npurposes it is always better to teach the commercial tool alongside the technical\nalternative. This demystifies the \"magic\" of AI and provides a free, secure backup for\nstudents with limited resources or high security needs. Commercial tools change pricing,\nalter privacy policies, or get banned. By teaching students the \"harder,\" technical route,\nyou give them a skill that is free, secure, and permanent. This is vital for investigative\njournalists handling sensitive leaks in hostile information environments.\n● 2) Time reinvestment. AI saves time. But in journalism, saving time is dangerous if that\ntime is treated as leisure. This toolkit introduces the concept of the \"Time Dividend.\" The\nRule: If an AI tool saves you 4 hours of transcribing, you owe the story 4 hours of deeper\nhuman work. The key to saving time with AI is also figuring out how to use that time to\nmake the piece of work better. What you should do with this extra time isn’t always\nobvious. And arguably it needs to be on tasks that can in no way be automated, like\ninterviewing one more source. The goal of saving time with AI is to spend more time on\nhuman connection and investigation. .\n● 3) Balance. To make it easier to compare AI tools in a newsroom or teaching context, this\ntoolkit introduces a simple, original rating system called the CDI score. The CDI score\nwas created specifically for this guide to help lecturers and students quickly assess\nwhether a tool is practical, accessible, and appropriate for their environment.\nRather than ranking tools as “good” or “bad,” the CDI score highlights barriers to entry.\nIt asks three questions every newsroom should consider before adopting an AI tool: Can\nwe afford it? Can we use it? And what does it cost us in terms of privacy and control?\n8 The State of Sovereign AI August\n9 AI, Data Governance and Privacy: Synergies and Areas of International Co-operation. June 20,\nprivacy_2ac13a42/2476b1a4-en.pdf"
}