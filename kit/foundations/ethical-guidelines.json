{
  "slug": "ethical-guidelines",
  "title": "Ethical Guidelines for the Sustainable Newsroom",
  "content": "Ethical Guidelines for the Sustainable Newsroom\nAs AI systems become embedded in reporting, editing, distribution and audience interaction, the\ncentral ethical challenge for journalism is no longer whether to use AI, but how to use it without\nsurrendering responsibility, credibility or source protection.\nThere is another issue which is linked to ethical AI and that is striving for independence from\nlarge tech companies. As you can imagine, this is easier said than done. Where does financial\ndependence on a certain platform start to infringe on your ethical standing. These guidelines are designed for a sovereign newsroom. In short, this means that one treats\nAI as infrastructure, not authority. You need to think of automation as assistance, not authorship\nand efficiency as a means to strengthen accountability, not weaken it.\nThe principles below establish non-negotiable red lines for editorial control, integrity and data\nsecurity. In this context, editorial control means that humans and not tools, platforms, or\nalgorithms retain final authority over journalistic decisions. Having a bedrock in the ethical\nprinciples is good preparation for when selecting and deciding on AI tools.\n1. The \"Human Command\" Principle 10\nThe Rule: AI is a tool for suggestion, never for decision. No AI output is published without\nhuman verification.\n● The \"Human-in-the-Loop\" workflow 11:\n○ Drafting vs. Publishing: AI may generate 100 headline ideas (Drafting), but a human\neditor must select the one that is accurate and fair (Publishing).\n○ The \"10-Second\" Audit: For every AI-generated summary or translation, the\njournalist must trace at least three claims back to the original source text. If the AI\ncannot provide a citation, the claim is treated as false until proven otherwise.\n○ The Kill Switch: Human editors must retain the technical ability to immediately retract\nor modify AI-driven content (automated chatbots) if they begin hallucinating or are\nhijacked by bad actors.\nTip: Ask students to treat AI like a \"Junior Intern\" who is incredibly fast but prone\nto pathological lying. You would never publish the intern’s work unread so then\ndo not publish the AI's.\n2. No \"Synthetic Reality\" 12\nThe Rule: Never use AI to simulate the physical reality of news events.\nIn conflict zones a photograph is evidence of a war crime. Using AI to generate \"realistic\"\nimages of protests, soldiers, or destruction—even for \"illustration\"—corrodes the public's ability to believe real photos.\n● Prohibited Uses (Red Lines):\n○ Generating images of specific people (politicians, activists) doing things they never\ndid.\n○ Generating \"photos\" of events (protests, bombings, meetings).\n○ Altering the content of real photos (for example, remove a person from a meeting).\n● Permitted Uses (Green Lines):\n○ Abstract Concepts: Visualizing non-physical stories like \"cyberwarfare,\" \"inflation,\"\nor \"mental health\" where no specific reality is being claimed.\n○ Clearly Stylized Art: Using AI to create cartoons or sketches that obviously look like\nart, not photography.\nBy flooding the zone with fake realistic images, we give dictators an excuse to dismiss\nreal evidence as \"just AI.\" Journalists must not contribute to this.3\n3. The \"Zero Trust\" Data Rule 13\nThe Rule: Assume every cloud tool is compromised. Protect your sources from your software.\nNewsrooms can face high-level state surveillance. Their Terms of Service often allow them to\nreview user data for \"safety\" or training purposes.\n- Green Data (Public Info): Press releases, published articles, public speeches.\n- Yellow Data (Internal/Low Risk): Drafts of opinion pieces, planning documents, non-\nsensitive interviews.\n- Red Data (Sensitive/Source Material): Whistleblower documents, interviews with\nsoldiers/victims and unreleased corruption evidence.\nThe"
}